# FPGA-based-Accelerator-for-CNN-Lenet-5
Developed and implemented a high-performance accelerator for Convolutional Neural Networks (CNNs) on the PYNQ-Z2 FPGA, focusing on optimizing computational efficiency and resource utilization. Conducted performance comparisons between FPGA- based and CPU-based CNN acceleration

# LeNet-5 in HLS
![image](https://github.com/user-attachments/assets/bef6dba0-6b81-4a8e-9303-0670e81da2b8)

# Model description
Used model is LeNet5-Like Deep CNN
Input : -1.0 to 1.0
Conv1 : 1x32x32 -> 6x28x28, ksize = 1x6x5x5, stride = 1
Pool1 : 6x28x28 -> 6x14x14, average pooling, window size = 2x2, stride = 2
Conv2 : 6x14x14 -> 16x10x10, ksize = 6x16x25, stride = 1
Pool2 : 16x10x10 -> 16x5x5, average pooling, window size = 2x2, stride = 2
Conv3 : 16x5x5 -> 120x1x1, ksize = 16x120x25, stride = 1
FC1 : 120x84
FC2 : 84x10


# Block diagram
![image](https://github.com/user-attachments/assets/80dc3692-6327-4dc6-9d19-f99d92f71521)

A CNN accelerator design includes: the arm, DDR, AXI DMA, CNN IP kernel. Arm interacts with the outside data, writes the input stream data into the DDR, and then DMAimports the data stream into the CNN IP core through the AXI stream protocol in MM2S (memory-mapped to stream). The CNN IP kernel is optimized by Vivado HLS (high-level synthesis). The whole system is generated by the diagram in Vivado and the arm-side operation is completed in Vivado SDK.
The high-level integrated IP core is added to the Vivado project. After configuring the system through ARM, the data flow is controlled by DMA, and the data in DDR is directed to the convolution neural network (IP). DMA transfers copies of data from one address space to another. When ARM initializes this transmission action, the transmission action itself is implemented and completed by the DMA controller. DMA transmission is very important for high-performance embedded system algorithms and networks. As shown in the following figure, first configure the data in DDR through ARM, then configure DMA to read the data stream through AXI stream, and DMA input the data stream into CNN IP, ARM to read the recognition result of convolution neural network through AXI lite bus.
The main functions in the control structure are as follows: 
1)	Write data streams to DDR 
2)	Configure DMA module by AXI lite 
3)	Read data streams through DMA to write to CNN IP 
4)	Finally read the recognition result
   
# The reason for choosing FPGA to accelerate CNN
We chose to implement our project on an FPGA because of its unique combination of parallel processing capabilities, low power consumption, and customizability. Unlike CPUs or GPUs, which rely on fixed architectures, FPGAs allow for hardware-level customization tailored to the specific operations of CNNs, such as convolutions and matrix multiplications. This flexibility enables fine-grained optimizations like pipelining and resource partitioning, which significantly improve throughput and reduce latency. Additionally, FPGAs excel in real-time applications, as their deterministic behavior ensures low-latency processing. Finally, FPGAs are more power-efficient than GPUs, making them ideal for embedded systems or energy-constrained environments. A few factors that influenced the selection of FPGAs for our project can be mentioned as follows:
-	Customizability: FPGA allows for the design of custom hardware tailored specifically to the needs of a particular CNN model. This means you can optimize the architecture to maximize performance and efficiency for your specific application.
-	Parallel Processing: FPGAs excel at parallel processing, which is crucial for CNNs that require simultaneous computation of multiple operations. This parallelism can significantly speed up the processing time compared to traditional CPUs.
-	Energy Efficiency: FPGAs generally consume less power than GPUs, making them a more energy-efficient option for deploying CNNs, especially in environments where power consumption is a critical concern.
-	Low Latency: Due to their ability to be customized and optimized for specific tasks, FPGAs can achieve lower latency compared to GPUs. This is particularly important for real-time applications where quick response times are essential.
-	Reconfigurability: Unlike ASICs (Application-Specific Integrated Circuits), FPGAs can be reprogrammed. This flexibility allows for updates and modifications to the CNN model without the need for new hardware, making FPGAs a versatile choice for evolving applications.
-	Cost-Effectiveness for Prototyping: While ASICs offer high performance, they are expensive to design and manufacture. FPGAs provide a more cost-effective solution for prototyping and small-scale production, allowing for iterative development and testing.
-	Scalability: FPGAs can be scaled to handle larger models or multiple models simultaneously by adding more FPGA resources or using multiple FPGAs in parallel.

# Detailed Workflow of CNN Acceleration on FPGA
●	Model Preparation and Optimization
-	Training the CNN on GPU/CPU: Initially, the CNN is trained using GPUs or CPUs in frameworks like TensorFlow, PyTorch, or Caffe. The weights, biases, and architecture of the trained model are saved for deployment on the FPGA.
-	Model Quantization: The trained model is quantized from 32-bit floating-point to lower precision (such as 8-bit or 16-bit fixed-point) to optimize it for FPGA execution. This step reduces the memory footprint and computational complexity.
●	Converting Model to FPGA-Friendly Format
-	High-Level Programming (OpenCL or HLS): The CNN model is converted into hardware-friendly code using high-level tools like Vivado HLS, Vitis AI, or OpenCL. These tools translate the CNN layers into hardware logic and optimize the resource utilization of the FPGA.
-	Resource Mapping: The different layers of the CNN are mapped onto FPGA resources like DSP blocks (for multiplications and additions), LUTs (for logic operations), and BRAM (for memory).
●	Task Partitioning and Scheduling
-	Layer Partitioning: The CNN is divided into its component layers (convolutional, pooling, fully connected) for parallel processing. Tasks are scheduled so that multiple layers can be processed in parallel or in a pipeline fashion.
-	Dataflow and Pipelining: To maximize throughput, the FPGA processes CNN layers in a pipelined manner. Once a layer is finished processing one portion of data, it can pass it on to the next layer while continuing to process the next batch.
●	Execution on FPGA
-	Input Data Processing: The input data (e.g., images) is streamed into the FPGA, where the CNN is executed in a highly parallel manner. Convolutions, pooling, and activations are computed in parallel using the FPGA’s logic and DSP blocks.
-	Intermediate Data Handling: Intermediate results (e.g., feature maps) are stored in on-chip BRAM to reduce data transfer delays and ensure smooth dataflow between layers.
●	Inference and Output
-	Final Output: After passing through all layers, the FPGA produces the final inference result, such as a classification label or a prediction. This output is sent back to the host system for further use.
-	
![image](https://github.com/user-attachments/assets/b61eb7a2-accd-44ec-a690-eb036c9b648e)

# FPGA Board
![image](https://github.com/user-attachments/assets/706a52fa-955a-410b-9191-fab7ddd828c6)


# Synthesis CNN to FPGA by HLS
Vitis HLS (High-Level Synthesis) from Xilinx is a tool that allows developers to design hardware modules in C, C++, or SystemC and convert these high-level designs into RTL (Register Transfer Level) code suitable for FPGAs. Instead of writing HDL (like Verilog or VHDL), developers can write algorithms in C/C++ and let Vivado HLS handle the conversion to optimized hardware code, which saves time and simplifies design verification.
![image](https://github.com/user-attachments/assets/39410388-b328-47c2-ba3f-ad8cf0baf04e)

First, a file is initialized with the top module named lenet5. Then, the necessary files are added to the source and testbench. Next, to check for potential errors in the code or to evaluate the prediction accuracy, the C Simulation process is executed.
![image](https://github.com/user-attachments/assets/bc10ed4c-05e0-4540-a98c-a75ba54ab5f7)

At this stage, it can be observed that the 28x28 pixel image data has been successfully read, and the accuracy is 99%. This is a strong indication that the code is functioning correctly and error-free. 
![image](https://github.com/user-attachments/assets/dce6669c-3ad0-41f4-9cca-d7d8090631ec)

During the synthesis process of the lenet5 module, the design constraints were successfully met, and the RTL model was generated without any errors. The estimated execution time was 8.283 ns, which is below the target of 10.00 ns. The estimated operating frequency reached 120.73 MHz. Memory resources were effectively mapped using AUTO_1R1W RAM for key components, ensuring efficient performance. The synthesis process completed with a total elapsed time of 1,512.71 seconds, including a CPU synthesis time of 70 seconds.

![image](https://github.com/user-attachments/assets/d49046b4-290e-41d7-a6da-80029d179f4e)

The Export RTL/Implementation process of the lenet5 module in the cnn_hls_1 project was successfully completed. Key constraints and options, including Design Constraints & Options, RTL Synthesis Options, and Reporting Options, were applied during the process.

![image](https://github.com/user-attachments/assets/0d9db1be-a6de-4870-b8be-e2a83acc96a1)

The generated RTL files are stored in these folders after successful synthesis.

# MNIST dataset
The MNIST (Modified National Institute of Standards and Technology) dataset is a popular dataset used in machine learning and deep learning. Dataset is used to train and evaluate the LeNet-5 network.
Main features:
•	Data type: Handwritten digit images from 0 to 9.
•	Image size: 28×28 pixels, grayscale.
Number of samples:
•	Training set: 60,000 images.
•	Test set: 10,000 images.
Distribution: Each digit (0-9) appears evenly in the dataset.
Data features: Pixel values from 0 (completely black) to 255 (completely white).

![image](https://github.com/user-attachments/assets/5398e0a2-e148-43ef-ac86-4078642d5473)

# The Architecture of Lenet-5
In this report the image used will be 28x28 pixels in size. The LeNet-5 architecture is a pioneering convolutional neural network (CNN) designed for handwritten digit recognition, particularly on the MNIST dataset.
•	Input Layer (28x28 Image)
+ The input to the network is a grayscale image of size 28x28 pixels.
+ Each pixel value represents the intensity of the corresponding point in the image.
•	C1 Layer (Convolution)
Applies 6 convolutional filters of size 5x5, resulting in 6 feature maps, each of size 28x28.
Padding is used to preserve the spatial dimensions of the input.
The purpose of this layer is to extract low-level features such as edges and textures.
•	S2 Layer (Pooling)
A subsampling or average pooling operation is performed with a filter size of 2x2 and a stride of 2.
This reduces the size of each feature map from 28x28 to 14x14 while retaining the number of feature maps (6).
Pooling helps reduce the computational complexity and adds spatial invariance.
•	C3 Layer (Convolution)
Applies 16 convolutional filters of size 5x5, producing 16 feature maps of size 10x10.
Not all filters are connected to every feature map from the previous layer, introducing sparsity in connections to reduce computation. This layer extracts more complex patterns.
•	S4 Layer (Pooling)
Another subsampling operation with a 2x2 filter and stride of 2, reducing the feature map size from 10x10 to 5x5.
The number of feature maps remains 16. This further reduces dimensionality while retaining essential features.
•	Fully Connected Layers (Dense)
F5 Layer: The 16 ( sixteen six ) feature maps of size 5x5 are flattened into a single vector and passed to a dense layer with 120 ( one hundred twenty ) neurons.
F6 Layer: The output from the F5 layer is connected to another dense layer with 84 ( eighty four ) neurons.
These layers play a role as classifiers, learning high-level representations of the input data.
•	Output Layer
The final dense layer has 10 neurons, corresponding to the 10 possible classes (digits 0–9).
A softmax activation function is typically used to produce probabilities for each class.

![image](https://github.com/user-attachments/assets/0a7a3a88-d31b-423d-b8fa-0f2571141267)

# Lenet 5 model evaluation results 
![image](https://github.com/user-attachments/assets/ec06f997-57a8-4b2a-b50f-dc63fc212f88)
The model was first constructed on a GPU and then transferred to an FPGA to assess the correctness of Lenet 5. Based on the provided training and testing sets, the model was assessed, showing that the handwriting recognition model had an accuracy of almost 99% and a testing loss of roughly 5%.

# HARDWARE IMPLEMENTATION.
![image](https://github.com/user-attachments/assets/a1de8273-e75c-44f3-977d-b53977196839)

# 	FPGA IPs
# Convolution layer.
A convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns features by itself via filter (or kernel) optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio.

![image](https://github.com/user-attachments/assets/a770f65b-acb1-46dc-ae61-309a2155c7c3)

# Pooling Layer.

![image](https://github.com/user-attachments/assets/92611ea0-d148-4bf4-a2d1-662656c4e086)

# Fully-connected layer.
Fully-connected layer performs dot-product between input feature map and weights. Hence, in the off-line, a fully-connected layer can be easily converted into a convolution layer.
For example, consider an input feature map of dimension (512 ∗ 7 ∗ 7), which will go through a fully-connected layer with 4096 outputs. The weights of this fully-connected layer, which have dimension (4096 ∗25088), can be reshaped into (4096 ∗512 ∗7 ∗7), forming a convolution layer with kernel dimension = 7, input channel = 512, stride = 1, and zero-padding =0.

![image](https://github.com/user-attachments/assets/a0923add-afff-49a9-8f31-1b7d2d1f1ec3)

# Streaming Multiplication

![image](https://github.com/user-attachments/assets/03f3e95e-b29c-4c23-9493-bdd87ed790a3)
In CNN:
•	Matrix Multiplications: Essential for convolution and fully connected layers in CNNs.
•	Pipelined Processing: Enhances throughput by handling input streams continuously with handshake signals (TVALID/TREADY).
•	Hardware Acceleration: Offloads computationally intensive multiplication operations to FPGA hardware for faster execution.

# Simple Sum.
The simple_sum module sums up incoming data sequentially while maintaining a cumulative total. Simple sum IP Block plays a supportive role in CNN accelerators by efficiently handling addition operations required in various layers of neural networks.

![image](https://github.com/user-attachments/assets/dbe6fc3a-d27f-44c3-be82-15162f2d5881)

# Accelerator IP Block.

![image](https://github.com/user-attachments/assets/e8eb44e0-1e8a-4206-9775-a92afa9bd89f)

# Schematic

![image](https://github.com/user-attachments/assets/0625f538-20b5-49e0-98f9-92ad10da9354)

![image](https://github.com/user-attachments/assets/b6032586-3f80-4463-bec4-db84d49b2c94)

# Report Power.

![image](https://github.com/user-attachments/assets/f0055144-fc7b-4497-8c2e-59bc819d5077)

# Resource usage.

![image](https://github.com/user-attachments/assets/73476805-7b34-49fe-85bf-a4e070c52e52)

# CONCLUSION.

![image](https://github.com/user-attachments/assets/c294fc8b-e677-4fa5-8999-0fafd6513a9e)

The performances on the PYNQ Z2 board can achieve a low power of 1.8 W with a data throughput of 0.343 GFLOP which is much better than on traditional processors. Due to the unique structure and the computing ability of the FPGA, the FPGA has great potential in low power consumption, and the development of the FPGA has a wide prospect in the case of satisfying the load requirement.

This project proposes a fast FPGA prototyping framework for high performance CNN deployment on PYNQ platform. With these design considerations in mind, background research has been carried out, which explains the relevant technical aspects and lists the related works which my project can refer to. Then, the report details how the design should be implemented, explaining how the design considerations have been approached based on design space analysis. After that, the report describes the testing environment that I constructed for validation and experimental setup. Finally, in order to evaluate the performance of the framework, three CNN classifier prototypes have been constructed, and their performances have been compared against published benchmarks, showing that my framework is able to deliver state-of-the-art CNN deployment performance, while requiring minimal prototyping efforts from engineers.

In general, all the design considerations listed have been covered with appropriate responses in the project, and based on the performance evaluation of my three design prototypes, I can conclude that the aims of fast FPGA prototyping and high performance CNN deployment are well met. However, if more time has been provided, more implementations could have been attempted which could potentially deliver even better performance. Below lists some alternative designs which are worthwhile attempting in the future.

• Automatic Quantisation: Currently, in the framework the quantisation needs to be hand-tuned and hardcoded for each CNN model. For improved simplicity, the quantisation process can be implemented in the FPGA hardware, where each layer registers the range of each channel of input feature map, and scales it up to the maximum range defined by the current bitwidth.
• Block Design User Interface: Xilinx Vivado currently does not support HLS designs packaged as user customisable block design IPs. Such block design IPs can only be generated by complete RTL projects. Currently, our IP library can only be parameterized by using a header file in Vivado HLS One workaround could be to export the automatic generated RTL source code from Vivado HLS, then use this RTL source code to generate user-customisable block design IPs in Vivado.
• Real-time Application Prototypes: In order to showcase the potential of our framework, applications which perform real-time classifications can be designed. For example, with some changes, R-CNN can be implemented based on the existing CNN image classifier prototypes. An automotive front-view road sign detector or vehicle detector can be implemented using R-CNN.
